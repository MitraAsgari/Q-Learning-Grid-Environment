# -*- coding: utf-8 -*-
"""Q-Learning Grid Navigation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iZQ3gMXiNuMscNMlygzmXFL-E9HRFyei
"""

# Import necessary libraries
import numpy as np
import random
import matplotlib.pyplot as plt

# Define the grid environment
class GridEnv:
    def __init__(self):
        self.grid_size = 4
        self.state_space = self.grid_size ** 2
        self.action_space = 4
        self.state = (0, 0)
        self.goal = (3, 3)
        self.reward = -1

    def reset(self):
        self.state = (0, 0)
        return self.state

    def step(self, action):
        x, y = self.state
        if action == 0:  # Move up
            x = max(x - 1, 0)
        elif action == 1:  # Move down
            x = min(x + 1, self.grid_size - 1)
        elif action == 2:  # Move left
            y = max(y - 1, 0)
        elif action == 3:  # Move right
            y = min(y + 1, self.grid_size - 1)

        self.state = (x, y)
        done = self.state == self.goal
        reward = 0 if done else self.reward
        return self.state, reward, done

# Define the Q-Learning agent
class QLearningAgent:
    def __init__(self, env):
        self.env = env
        self.q_table = np.zeros((env.state_space, env.action_space))
        self.alpha = 0.1
        self.gamma = 0.9
        self.epsilon = 0.1

    def get_state_index(self, state):
        x, y = state
        return x * self.env.grid_size + y

    def choose_action(self, state):
        if random.uniform(0, 1) < self.epsilon:
            return random.choice(range(self.env.action_space))
        else:
            state_idx = self.get_state_index(state)
            return np.argmax(self.q_table[state_idx])

    def update_q_table(self, state, action, reward, next_state):
        state_idx = self.get_state_index(state)
        next_state_idx = self.get_state_index(next_state)
        best_next_action = np.argmax(self.q_table[next_state_idx])
        td_target = reward + self.gamma * self.q_table[next_state_idx][best_next_action]
        td_error = td_target - self.q_table[state_idx][action]
        self.q_table[state_idx][action] += self.alpha * td_error

# Training the Q-Learning agent
def train_agent(env, agent, episodes):
    rewards = []
    for _ in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False
        while not done:
            action = agent.choose_action(state)
            next_state, reward, done = env.step(action)
            agent.update_q_table(state, action, reward, next_state)
            state = next_state
            total_reward += reward
        rewards.append(total_reward)
    return rewards

# Initialize the environment and agent
env = GridEnv()
agent = QLearningAgent(env)

# Train the agent
episodes = 1000
rewards = train_agent(env, agent, episodes)

# Plotting the training rewards
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Progress')
plt.show()

# Display the Q-table
print("Q-table:")
print(agent.q_table.reshape((env.grid_size, env.grid_size, env.action_space)))